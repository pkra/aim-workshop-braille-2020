<html><head><title>No Title</title></head><body>
<p>Contents . . .. . .. . .. . .. . .. . .. . .. . .. . .' 3</p>

<p>Notation . . .. . .. . .. . .. . .. . .. . .. . .. . . 5</p>

<p>Definition . . .. . .. . .. . .. . .. . .. . .. . .'' 6</p>

<p>Illustration . . .. . .. . .. . .. . .. . .. . .' 8</p>

<p>Fundamental applications . . .. . .. . .. . .' 9</p>

<p>Linear maps . . .. . .. . .. . .. . .. . .. . .' 9</p>

<p>System of linear equations . . .. . .. . .' 11</p>

<p>Dot product, bilinear form and inner</p>

<p>product . . .. . .. . .. . .. . .. . .. . .' 11</p>

<p>General properties . . .. . .. . .. . .. . .'' 13</p>

<p>Non-commutativity . . .. . .. . .. . .'' 13</p>

<p>Distributivity . . .. . .. . .. . .. . .. . .' 15</p>

<p>Product with a scalar . . .. . .. . .. . .' 15</p>

<p>Transpose . . .. . .. . .. . .. . .. . .. . . 16</p>

<p>Complex conjugate . . .. . .. . .. . .. . . 17</p>

<p>Associativity . . .. . .. . .. . .. . .. . . 18</p>

<p>Complexity is not associative '' 19</p>

<p>Application to similarity . . .. . .' 20</p>

<p>Square matrices . . .. . .. . .. . .. . .. . . 22</p>

<p>Powers of a matrix . . .. . .. . .. . .. . . 23</p>

<p>Abstract algebra . . .. . .. . .. . .. . .'' 25</p>

<p>Computational complexity . . .. . .. . .'' 27</p>

<p>Related complexities . . .. . .. . .'' 28</p>

<p>Matrix inversion, determinant and</p>

<p>Gaussian elimination . . .. . .' 30  1</p>

<p>See also . . .. . .. . .. . .. . .. . .. . .. . .'' 34</p>

<p>Notes . . .. . .. . .. . .. . .. . .. . .. . .. . . 35</p>

<p>References . . .. . .. . .. . .. . .. . .. . .. . .' 41</p>

<p>2	 matrix multiplication, the number of columns in the first matrix
must be equal to the number of rows in the second matrix. The result
matrix has the number of rows of the first and the number of columns
of the second matrix.</p>

<p>In mathematics, ~1matrix ~1multiplication is a binary operation that
produces a matrix from two matrices. For matrix multiplication, the
number of columns in the first matrix must be equal to the number of
rows in the second matrix. The result matrix, known as the ~1matrix
~1product~', has the number of rows of the first and the number of
columns of the second matrix.</p>

<p>Matrix multiplication was first described by the French mathematician
Jacques Philippe Marie Binet in 1812, .ghblea.ar  to represent the
composition of linear maps that are represented by matrices. Matrix
multiplication is thus a basic tool of linear algebra, and as such has
numerous applications in many areas of mathematics, as well as in
applied mathematics, statistics, physics, economics, and engineering.
.ghbleb.ar .ghblec.ar  Computing matrix products is a central
operation in all computational applications of linear algebra.</p>

<p>2</p>

<p>Contents</p>

<p> 1 Notation 2 Definition 241 Illustration 3 Fundamental applications
341 Linear maps 342 System of linear equations 343 Dot product,
bilinear form and inner</p>

<p>product 4 General properties 441 Non-commutativity 442 Distributivity
443 Product with a scalar 444 Transpose 445 Complex conjugate 446
Associativity 44641 Complexity is not associative 44642 Application to
similarity 5 Square matrices 541 Powers of a matrix 6 Abstract algebra
7 Computational complexity 741 Related complexities 742 Matrix
inversion, determinant and</p>

<p>Gaussian elimination 8 See also 9 Notes 10 References</p>

<p>4</p>

<p>Notation</p>

<p>This article will use the following notational conventions: matrices
are represented by capital letters in bold, eddg.  ~2A , vectors in
lowercase bold, eddg.  ~2a , and entries of vectors and matrices are
italic "ghsince they are numbers from a field"ar, eddg.  A  and  a .
Index notation is often the clearest way to express definitions, and
is used as standard in the literature. The  I, j  entry of matrix ~2A
is indicated by "gh~2A"ar ij ,  A ij or  a ij , whereas a numerical
label "ghn matrix entries"ar on a collection of matrices is
subscripted only, eddg.  ~2A 1, ~2a 2, etc.</p>

<p>5</p>

<p>Definition</p>

<p>If  ~2A  is an  m "" n  matrix and ~2B  is an  n "" p  matrix,</p>

<p>_sh _A = Ofa,, a,; . . . a;1n"With Ofa;, a;; . . . a;ben"With Ofgh. .
. gh. . . . . . gh. . .With Ofa;more"," a;more";" . . . a;mn"With _B =
Ofb,, but,; . . . b;1p"With Ofb;, but;; . . . b;bep"With Ofgh. . . gh.
. . . . . gh. . .With Ofbtion"," btion";" . . . btionp"With</p>

<p>the matrix product  ~2C "( ~1ab "ghdenoted without multiplication
signs or dots"ar is defined to be the  m "" p  matrix .ghbled.ar
.ghblee.ar .ghblef.ar .ghbleg.ar</p>

<p>_sh _C = Ofc,, can,; . . . c;1p"With Ofc;, can;; . . . c;bep"With
Ofgh. . . gh. . . . . . gh. . .With Ofc;more"," c;more";" . . .
c;mpWith</p>

<p>such that</p>

<p>_sh c;ij" = a;i"1"b;1j"+a;i"be"b;bej"+. . .+a;in"btionj" = ".Sshk =
1ghner1ik"b;kj"</p>

<p>for  I "( 1, ..., m  and  j "( 1, ..., p .</p>

<p>That is, the entry _sh c;ij  of the product is obtained by multiplying
term-by-term the entries of the ith row of  ~2A  and the jth column of
 ~2B , and summing these n products. In other words, _sh c;ij  is the
dot product of the ith row of  ~2A  and the jth column of  ~2B .</p>

<p>Therefore,  ~1AB  can also be written as</p>

<p>_sh _C = Ofaeaeabeaeaing. . .+a;1n"btion"," aeaeabeabbing. .
.+a;1n"btion";" . . . aeaeab;1p"+. . .+a;1n"btionp"With
Ofabbeabeaeaing. . .+a;ben"btion"," abbeabeabbing. . .+a;ben"btion";"
. . . abbeab;1p"+. . .+a;ben"btionp"With Ofgh. . . gh. . . . . . gh. .
.With Ofa;m"1"beaeaing. . .+a;mn"btion"," a;m"1"beabbing. .
.+a;mn"btion";" . . . a;m"1"b;1p"+. . .+a;mn"btionpWith</p>

<p>Thus the product  ~1AB  is defined if and only if the number of
columns in  ~2A equals the number of rows in  ~2B , in this case n .</p>

<p>Usually the entries are numbers, but they may be any kind of
mathematical objects for which an addition and a multiplication are
defined, that are associative, and such that the addition is
commutative, and the multiplication is distributive with respect to
the addition. In particular, the entries may be matrices themselves
"ghsee block matrix"ar.</p>

<p>Illustration</p>

<p>The figure to the right illustrates diagrammatically the product of
two matrices  ~2A  and  ~2B , showing how each intersection in the
product matrix corresponds to a row of  ~2A  and a column of  ~2B .</p>

<p>_sh "`Ofa,, aeabb`With `Ofch ch`With `Ofa:, accbb`With `Ofch
ch`Withghbledd·; matrixer"`Ofch but,; beacc`With `Ofch but;;
bbbcc`Withghblebb·: matrixer = "`Ofch can,; ceacc`With `Ofch child
ch`With `Ofch can:; ccccc`With `Ofch child ch`Withghbledd·: matrixer</p>

<p>The values at the intersections marked with circles are:</p>

<p>_sh can,;  = aeaeabeabbingaeabbb;; can::  = acceabeaccingaccbbb;:</p>

<p>8</p>

<p>Fundamental applications</p>

<p>Historically, matrix multiplication has been introduced for making
easier and clarifying computations in linear algebra. This strong
relationship between matrix multiplication and linear algebra remains
fundamental in all mathematics, as well as in physics, engineering and
computer science.</p>

<p>Linear maps</p>

<p>If a vector space has a finite basis, its vectors are each uniquely
represented by a finite sequence of scalars, called a coordinate
vector, whose elements are the coordinates of the vector on the basis.
These coordinate vectors form another vector space, which is
isomorphic to the original vector space. A coordinate vector is
commonly organized as a column matrix "ghal called column vector"ar,
which is a matrix with only one column. So, a column vector represents
both a coordinate vector, and a vector of the original vector space.</p>

<p>A linear map A from a vector space of dimension n into a vector space
of dimension m maps a column vector</p>

<p>_sh _x = it, it; gh. . . xtion</p>

<p>onto the column vector</p>

<p>_sh _y = Aof_xwith = aeaeaxeaing. . .+a;1n"xtion" abbeaxeaing. .
.+a;ben"xtion" gh. . . a;m"1"xeaing. . .+a;mn"xtion"."</p>

<p>The linear map A is thus defined by the matrix</p>

<p>_sh _A = Ofa,, a,; . . . a;1n"With Ofa;, a;; . . . a;ben"With Ofgh. .
. gh. . . . . . gh. . .With Ofa;more"," a;more";" . . . a;mn"With</p>

<p>and maps the column vector _sh _x  to the matrix product</p>

<p>_sh _y = _A_x."</p>

<p>If B is another linear map from the preceding vector space of
dimension m, into a vector space of dimension p, it is represented by
a _sh p·m  matrix _sh _B."  A straightforward computation shows that
the matrix of the composite map _sh B~.chA  is the matrix product _sh
_B_A."  The general formula _sh ofB~.chAwithof_xwith =
BofAof_xwithwith "ar that defines the function composition is
instanced here as a specific case of associativity of matrix product
"ghsee ~s Associativity, bel"ar:</p>

<p>_sh of_B_Awith_x = _Bof_A_xwith = _B_A_x."</p>

<p>System of linear equations</p>

<p>The general form of a system of linear equations is</p>

<p>_sh aeaeaxeaing. . .+a;1n"xtion" = but, abbeaxeaing. . .+a;ben"xtion"
= but; gh. . . a;m"1"xeaing. . .+a;mn"xtion" = b;more"."</p>

<p>Using same notation as above, such a system is equivalent with the
single matrix equation</p>

<p>_sh _A_x = _b."</p>

<p>Dot product, bilinear form and inner product</p>

<p>The dot product of two column vectors is the matrix product</p>

<p>_sh _x~.T"_y</p>

<p>where _sh _x~.That  is the row vector obtained by transposing _sh _x
and the resulting 1"81 matrix is identified with its unique entry.</p>

<p>More generally, any bilinear form over a vector space of finite
dimension may be expressed as a matrix product</p>

<p>_sh _x~.T"_A_y</p>

<p>and any inner product may be expressed as</p>

<p>_sh _x~_er"_A_y</p>

<p>where _sh _x~_er  denotes the conjugate transpose of _sh _x
"ghccjugate of the transpose, or equivalently transpose of the
conjugate"ar.</p>

<p>12</p>

<p>General properties</p>

<p>Matrix multiplication shares some properties with usual
multiplication. However, matrix multiplication is not defined if the
number of columns of the first factor differs from the number of rows
of the second factor, and it is non-commutative, even when the product
remains definite after changing the order of the factors. .ghbleh.ar
.ghblei.ar</p>

<p>Non-commutativity</p>

<p>An operation is commutative if, given two elements  ~2A  and  ~2B
such that the product _sh _A_B  is defined, then _sh _B_A  is also
defined, and _sh _A_B = _B_A."</p>

<p>If  ~2A  and  ~2B  are matrices of respective sizes _sh m·n  and _sh
p·q , then _sh _A_B  is defined if _sh not = people , and _sh _B_A  is
defined if _sh more = quite . Therefore, if one of the products is
defined, the other is not defined in general. If _sh more = quite st=
not = people , the two products are defined, but have different sizes;
thus they cannot be equal.  Only if _sh more = quite = not = people ,
that is if  ~2A  and  ~2B  are square matrices of the same size are
both products defined and the same size. Even in this case, one has in
general</p>

<p>_sh _A_B st= _B_A."</p>

<p>For example</p>

<p>_sh Ofble" 1With Of" by WithOf" by With Of, by With = Ofble, by With
Of" by With</p>

<p>but</p>

<p>_sh Ofble" by With Of, by WithOf" 1With Of" by With = Ofble" by With
Of" 1With."</p>

<p>This example may be expanded for showing that, if ~2A  is a _sh n·n
matrix with entries in a field F, then _sh _A_B = _B_A  for every _sh
n·n  matrix  ~2B  with entries in F, if and only if _sh _A = c_I
where _sh can è From , and  ~2I  is the _sh n·n  identity matrix. If,
instead of a field, the entries are supposed to belong to a ring, then
one must add the condition that c belongs to the center of the ring.</p>

<p>One special case where commutativity does occur is when  ~2D  and  ~2E
 are two "ghsquare"ar diagonal matrices "ghof the same size"ar; then
~1De "( ~1ed . Again, if the matrices are over a general ring rather
than a field, the corresponding entries in each must also commute with
each other for this to hold.</p>

<p>Distributivity</p>

<p>The matrix product is distributive with respect to matrix addition.
That is, if ~2A, ~2b, ~2c, ~2d  are matrices of respective sizes  m ""
n ,  n "" p ,   n "" p , and  p "" q , one has "ghleft
distributivity"ar</p>

<p>_sh _Aof_Bing_Cwith = _A_Bing_A_C</p>

<p>and "ghright distributivity"ar</p>

<p>_sh of_Bing_Cwith_D = _B_Ding_C_D."</p>

<p>This results from the distributivity for coefficients by</p>

<p>_sh ".Sshkera;ik"ofb;kj"+c;kj"with =
".Sshkera;ik"b;kj"+".Sshkera;ik"c;kj  _sh
".Sshkerofb;ik"+c;ik"withd;kj" =
".Sshkerb;ikday;kj"+".Sshkerc;ikday;kj"."</p>

<p>Product with a scalar</p>

<p>15</p>

<p>If  ~2A  is a matrix and c a scalar, then the matrices _sh c_A  and
_sh _Ac  are obtained by left or right multiplying all entries of  ~2A
 by c. If the scalars have the commutative property, then _sh c_A =
_Ac."</p>

<p>If the product _sh _A_B  is defined "ght is the number of columns of
~2A  equals the number of rows of  ~2B , then</p>

<p>_sh cof_A_Bwith = ofc_Awith_B  and _sh of_A_Bwithc = _Aof_Bcwith."</p>

<p>If the scalars have the commutative property, then all four matrices
are equal. More generally, all four are equal if  c  belongs to the
center of a ring containing the entries of the matrices, because in
this case  c ~2X "( ~2x c  for all matrices  ~2X .</p>

<p>These properties result from the bilinearity of the product of
scalars:</p>

<p>_sh cof".Sshkera;ik"b;kj"with = ".Sshkerofca;ik"withb;kj  _sh
of".Sshkera;ik"b;kj"withc = ".Sshkera;ik"ofb;kj"cwith."</p>

<p>Transpose</p>

<p>If the scalars have the commutative property, the transpose of a
product of matrices is the product, in the reverse order, of the
transposes of the factors. That is</p>

<p>_sh of_A_Bwith~.That" = _B~.T"_A~.That</p>

<p>where T denotes the transpose, that is the interchange of rows and
columns.</p>

<p>This identity does not hold for noncommutative entries, since the
order between the entries of  ~2A and  ~2B  is reversed, when one
expands the definition of the matrix product.</p>

<p>Complex conjugate</p>

<p>If  ~2A  and  ~2B  have complex entries, then</p>

<p>_sh of_A_Bwith~`" = _A~`"_B~`</p>

<p>where  "in denotes the entry-wise complex conjugate of a matrix.</p>

<p>This results from applying to the definition of matrix product the
fact that the conjugate of a sum is the sum of the conjugates of the
summands and the conjugate of a product is the product of the
conjugates of the factors.</p>

<p>Transposition acts on the indices of the entries, while conjugation
acts independently on the entries themselves. It results that, if  ~2A
 and ~2B  have complex entries, one has</p>

<p>_sh of_A_Bwith~_er" = _B~_er"_A~_er"</p>

<p>where  `This denotes the conjugate transpose "ghccjugate of the
transpose, or equivalently transpose of the conjugate"ar.</p>

<p>Associativity</p>

<p>Given three matrices  ~2A, ~2B  and ~2C , the products "gh~1AB~'"ar~2C
 and ~2A"gh~1BC~'"ar are defined if and only if the number of columns
of  ~2A  equals the number of rows of  ~2B   and the number of columns
of ~2B  equals the number of rows of  ~2C "ghin particular, if one of
the products is defined, the other is also defined"ar. In this case,
one has the associative property</p>

<p>_sh of_A_Bwith_C = _Aof_B_Cwith."</p>

<p>As for any associative operation, this allows omitting parentheses,
and writing the above products as _sh _A_B_C."</p>

<p>This extends naturally to the product of any number of matrices
provided that the dimensions match. That is, if  ~2A 1, ~2a 2, ...,
~2a n  are matrices such that the number of columns of  ~2A I  equals
the number of rows of  ~2A I "! 1 for  I "( 1, ..., n - 1, then the
product</p>

<p>_sh "'ouxbbbb0f'shi = 1ghner_A;I" = _Aea_A; . . . _Ation"</p>

<p>is defined and does not depend on the order of the multiplications, if
the order of the matrices is kept fixed.</p>

<p>These properties may be proved by straightforward but complicated
summation manipulations. This result also follows from the fact that
matrices represent linear maps. Therefore, the associative property of
matrices is simply a specific case of the associative property of
function composition.</p>

<p>Complexity is not associative</p>

<p>Although the result of a sequence of matrix products does not depend
on the order of operation "ghprovided that the order of the matrices
is not changed"ar, the computational complexity may depend
dramatically on this order.</p>

<p>For example, if  ~2A, ~2B  and  ~2C are matrices of respective sizes
10"830, 30"85, 5"860, computing "gh~1AB~'"ar~2C  needs 10"830"85 "!
10"85"860 "( 4,ejj multiplications, while computing  ~2A"gh~1BC~'"ar
needs 30"85"860 "! 10"830"860 "( 27,jjj multiplications.</p>

<p>Algorithms have been designed for choosing the best order of products,
see Matrix chain multiplication. When the number n of matrices
increases, it has been shown that the choice of the best order has a
complexity of _sh Oofnlog nwith."</p>

<p>Application to similarity</p>

<p>Any invertible matrix _sh _P  defines a similarity transformation
"ghon square matrices of the same size as _sh _P "ar</p>

<p>_sh S;_P"of_Awith = _P~-1"_A_P."</p>

<p>Similarity transformations map product to products, that is</p>

<p>_sh S;_P"of_A_Bwith = S;_P"of_AwithS;_P"of_Bwith."</p>

<p>In fact, one has</p>

<p>_sh _P~-1"of_A_Bwith_P = _P~-1"_Aof_P_P~-1"with_B_P =
of_P~-1"_A_Pwithof_P~-1"_B_Pwith."</p>

<p>21</p>

<p>Square matrices</p>

<p>Let us denote _sh `Mtion"ofRwith  the set of not"? n  square matrices
with entries in a ring R, which, in practice, is often a field.</p>

<p>In _sh `Mtion"ofRwith , the product is defined for every pair of
matrices. This makes _sh `Mtion"ofRwith  a ring, which has the
identity matrix  ~2I  as identity element "ghthe matrix whose diagonal
entries are equal to 1 and all other entries are 0"ar. This ring is
also an associative R-algebra.</p>

<p>If  n `ar 1, many matrices do not have a multiplicative inverse. For
example, a matrix such that all entries of a row "ghor a column"ar are
0 does not have an inverse. If it exists, the inverse of a matrix  ~2A
 is denoted  ~2A "comblea, and, thus verifies</p>

<p>_sh _A_A~-," = _A~-1"_A = _I."</p>

<p>A matrix that has an inverse is an invertible matrix. Otherwise, it is
a singular matrix.</p>

<p>A product of matrices is invertible if and only if each factor is
invertible. In this case, one has</p>

<p>_sh of_A_Bwith~-," = _B~-1"_A~-,"."</p>

<p>When R is commutative, and, in particular, when it is a field, the
determinant of a product is the product of the determinants. As
determinants are scalars, and scalars commute, one has thus</p>

<p>_sh det of_A_Bwith = det of_B_Awith = det of_Awithdet of_Bwith."</p>

<p>The other matrix invariants do not behave as well with products.
Nevertheless, if R is commutative, _sh _A_B  and _sh _B_A  have the
same trace, the same characteristic polynomial, and the same
eigenvalues with the same multiplicities. However, the eigenvectors
are generally different if _sh _A_B st= _B_A."</p>

<p>Powers of a matrix</p>

<p>One may raise a square matrix to any nonnegative integer power
multiplying it by itself repeatedly in the same way as for ordinary
numbers. That is,</p>

<p>_sh _A~"" = _I    _sh _A~," = _A    _sh _A~k" = ""_A_A . . .
_Ash'ouxbbccdf'ershk timeser."</p>

<p>Computing the kth power of a matrix needs  k - 1 times the time of a
single matrix multiplication, if it is done with the trivial algorithm
"ghrepeated multiplication"ar. As this may be very time consuming, one
generally prefers using exponentiation by squaring, which requires
less than 2 log 2 k  matrix multiplications, and is therefore much
more efficient.</p>

<p>An easy case for exponentiation is that of a diagonal matrix. Since
the product of diagonal matrices amounts to simply multiplying
corresponding diagonal elements together, the  k th power of a
diagonal matrix is obtained by raising the entries to the power  k :</p>

<p>_sh Ofa,, was . . . by With Of" a;; . . . by With Ofgh. . . gh. . . .
. . gh. . .With Of" was . . . ationn"With~k" = Ofaeaea~k" was . . . by
With Of" abbbb~k" . . . by With Ofgh. . . gh. . . . . . gh. . .With
Of" was . . . ationn~k"With."</p>

<p>24</p>

<p>Abstract algebra</p>

<p>The definition of matrix product requires that the entries belong to a
semiring, and does not require multiplication of elements of the
semiring to be commutative. In many applications, the matrix elements
belong to a field, although the tropical semiring is also a common
choice for graph shortest path problems. .ghbleaj.ar  Even in the case
of matrices over fields, the product is not commutative in general,
although it is associative and is distributive over matrix addition.
The identity matrices "ghwh are the square matrices whose entries are
zero outside of the main diagonal and 1 on the main diagonal"ar are
identity elements of the matrix product. It follows that the  n "" n
matrices over a ring form a ring, which is noncommutative except if  n
"( 1 and the ground ring is commutative.</p>

<p>A square matrix may have a multiplicative inverse, called an inverse
matrix. In the common case where the entries belong to a commutative
ring r, a matrix has an inverse if and only if its determinant has a
multiplicative inverse in r. The determinant of a product of square
matrices is the product of the determinants of the factors. The n "" n
 matrices that have an inverse form a group under matrix
multiplication, the subgroups of which are called matrix groups. Many
classical groups "ghincluding all finite groups"ar are isomorphic to
matrix groups; this is the starting point of the theory of group
representations.</p>

<p>26</p>

<p>Computational complexity</p>

<p> For implementation techniques "ghin particular parallel and
distributed algorithms"ar, see Matrix multiplication algorithm.
The bound on .will over time</p>

<p>The matrix multiplication algorithm that results of the definition
requires, in the worst case, _sh not~:  multiplications of scalars and
_sh ofn-1withn~;  additions for computing the product of two square
not"? n  matrices. Its computational complexity is therefore _sh
Oofn~3"with , in a model of computation for which the scalar
operations require a constant time "ghin practice, this is the case
for floating point numbers, but not for integers"ar.</p>

<p>Rather surprisingly, this complexity is not optimal, as shown in 1969
by Volker Strassen, who provided an algorithm, now called Strassen's
algorithm, with a complexity of _sh Oofn~log~;
("with'ouxbbbbdd8'Oofn~2.807"with."  The exponent appearing in the
complexity of matrix multiplication has been improved several times,
leading to  Coppersmith-Winograd algorithm with a complexity of  O"gh
n 24376"ar "ghbleaiij"ar. .ghbleaa.ar  This algorithm has been
slightly improved in 2013 by Virginia Vassilevska Williams to a
complexity of  O"gh n 243729"ar and in 2014 by Fran~andcois Le Gall,
for a final "ghup to date"ar complexity of  O"gh n 243728639"ar.
.ghbleab.ar</p>

<p>The greatest lower bound for the exponent of matrix multiplication
algorithm is generally called _sh .O . One has _sh be knowwh .O ,
because one has to read the _sh not~;  elements of a matrix for
multiplying it by another matrix. Thus _sh be knowwh .O know ;.:(: .
It is unknown whether _sh be know .O . The largest known lower bound
for matrix-multiplication complexity is .w"gh n 2 log"gh n"ar"ar, for
a restricted kind of arithmetic circuits, and is due to Ran Raz.
.ghbleac.ar</p>

<p>Related complexities</p>

<p>The importance of the computational complexity of matrix
multiplication relies on the facts that many algorithmic problems may
be solved by means of matrix computation, and most problems on
matrices have a complexity which is either the same as that of matrix
multiplication "ghup to a multiplicative constant"ar, or may be
expressed in term of the complexity of matrix multiplication or its
exponent _sh .O."</p>

<p>There are several advantages of expressing complexities in terms of
the exponent _sh .O  of matrix multiplication. Firstly, if _sh .O  is
improved, this will automatically improve the known upper bound of
complexity of many algorithms. Secondly, in practical implementations,
one never uses the matrix multiplication algorithm that has the best
asymptotical complexity, because the constant hidden behind the big O
notation is too large for making the algorithm competitive for sizes
of matrices that can be manipulated in a computer. Thus expressing
complexities in terms of _sh .O  provide a more realistic complexity,
since it remains valid whichever algorithm is chosen for matrix
computation.</p>

<p>Problems that have the same asymptotic complexity as matrix
multiplication include determinant, matrix inversion, Gaussian
elimination "ghsee next section"ar. Problems with complexity that is
expressible in terms of _sh .O  include characteristic polynomial,
eigenvalues "ghb not eigenvectors"ar, Hermite normal form, and Smith
normal form.</p>

<p>Matrix inversion, determinant and Gaussian</p>

<p>elimination</p>

<p>In his 1969 paper, where he proved the complexity _sh Oofn~2.807"with
for matrix computation, Strassen proved also that Matrix inversion,
determinant and Gaussian elimination have, up to a multiplicative
constant, the same computational complexity as matrix multiplication.
The proof does not make any assumptions on matrix multiplication that
is used, except that its complexity is _sh Oofn~.o"with  for some _sh
.O >wh be</p>

<p>The starting point of Strassen's proof is using block matrix
multiplication. Specifically, a matrix of even dimension 2 n"82 n  may
be partitioned in four  not"? n blocks</p>

<p>_sh `OfA B`With `OfC D`With."</p>

<p>Under this form, its inverse is</p>

<p>_sh `OfA B`With `OfC D`With~-," =
`OfA~-1"+A~-1"BofD-CA~-1"Bwith~-1"CA~-,"
comA~-1"BofD-CA~-1"Bwith~-1"`With `Of-ofD-CA~-1"Bwith~-1"CA~-,"
ofD-CA~-1"Bwith~-1"`With</p>

<p>provided that A and _sh Do-CA~-1"But  are invertible.</p>

<p>Thus, the inverse of a 2 n"82 n  matrix may be computed with two
inversions, six multiplications and four additions or additive
inverses of  not"? n  matrices. It follows that, denoting respectively
by  I"gh n"ar,  M"gh n"ar and  A"gh n"ar "( n 2 the number of
operations needed for inverting, multiplying and adding not"? n
matrices, one has</p>

<p>_sh Iofblebbnwith knowwh beIofnwithingffMofnwithingddAofnwith."</p>

<p>If _sh not = be~k"  one may apply this formula recursively:</p>

<p>_sh Iofblebb~k"with  knowwh
beIofblebb~k-1"withingffMofblebb~k-1"withingddAofblebb~k-1"with</p>

<p>knowwh be~2"Iofblebb~k-be"withingffofMofblebb~k-1"withingbbM-
31ofblebb~k-be"withwithingddofAofblebb~k-1"withingbbAofblebb~k-be"withwith
. . .</p>

<p>If _sh Mofnwith knowwh cn~.O"  and _sh .a = be~.o" >wh .  one gets
eventually</p>

<p>_sh Iofblebb~k"with  knowwh
be~k"Iofbleeawithingffcof.a~k-1"+2.a~k-be"+. .
.+2~k-1".a~0"withingkbb~king,"    knowwh
be~k"+63th.a~k"combb~k"st.a-bebleingkbb~king,"    knowwh
dofbb~k"with~.O"."</p>

<p>for some constant d.</p>

<p>For matrices whose dimension is not a power of two, the same
complexity is reached by increasing the dimension of the matrix to a
power of two, by padding the matrix with rows and columns whose
entries are 1 on the diagonal and 0 elsewhere.</p>

<p>This proves the asserted complexity for matrices such that all
submatrices that have to be inverted are indeed invertible. This
complexity is thus proved for almost all matrices, as a matrix with
randomly chosen entries is invertible with probability one.</p>

<p>The same argument applies to LU decomposition, as, if the matrix A is
invertible, the equality</p>

<p>_sh `OfA B`With `OfC D`With = `OfI by `With `OfCA~-," I`With`OfA
B`With `Of" Do-CA~-1"B`With</p>

<p>defines a block LU decomposition that may be applied recursively to
_sh A  and _sh Do-CA~-1"But  for getting eventually a true LU
decomposition of the original matrix.</p>

<p>The argument applies also for the determinant, since it results from
the block LU decomposition that</p>

<p>_sh det `OfA B`With `OfC D`With = det ofAwithdet ofD-CA~-1"Bwith."</p>

<p>33</p>

<p>See also</p>

<p> Matrix calculus, for the interaction of matrix</p>

<p>multiplication with operations from calculus Other types of products
of matrices: Block matrix multiplication Cracovian product, defined as
 ~2A "</p>

<p>~2b "( ~2b t~2a Frobenius inner product, the dot product of</p>

<p>matrices considered as vectors, or,</p>

<p>equivalently the sum of the entries of the</p>

<p>Hadamard product Hadamard product of two matrices of the same</p>

<p>size, resulting in a matrix of the same</p>

<p>size, which is the product entry-by-entry Kronecker product or tensor
product, the</p>

<p>generalization to any size of the preceding Outer product, also called
dyadic product or</p>

<p>tensor product of two column matrices, which</p>

<p>is _sh _a_b~.That</p>

<p>34</p>

<p>Notes</p>

<p> O'Connor, John J.; Robertson,</p>

<p>Edmund F., "Jacques Philippe Marie</p>

<p>Binet",  MacTutor History of</p>

<p>Mathematics archive , University of St</p>

<p>Andrews    dismw-parser-output</p>

<p>citeddcitation^font-style;coninherit_arddmw-par-</p>

<p>ser-output discitation</p>

<p>q^quotescc[_ch[[[_ch[[['[['[_-</p>

<p>arddmw-parser-output disid-lock-free</p>

<p>aeaddmw-parser-output discitation</p>

<p>discsblea-lock-free</p>

<p>a^background-image;conurl"gh[//upload.-</p>

<p>wikimediaddorg/wikipedia/commons/thumb-</p>

<p>/6/65/Lock-greenddsvg/9px-Lock-</p>

<p>comgreenddsvgddpng["arbbbackground-image;conline-</p>

<p>ar-gradient"ghtransparent;1transparent"areaurl-</p>

<p>"gh[//uploadddwikimediaddorg/wikipedia-</p>

<p>/commons/6/65/Lock-greenddsvg["-</p>

<p>arbbbackground-repeat;conno-repeat;bebackground --</p>

<p>sizeccbleipxbbbackground-position;conright dis1em</p>

<p>center_arddmw-parser-output disid-lock-limited</p>

<p>aeaddmw-parser-output disid-lock-registration</p>

<p>aeaddmw-parser-output discitation</p>

<p>discsblea-lock-limited aeaddmw-parser-output</p>

<p>discitation discsblea-lock-registration</p>

<p>a^background-image;conurl"gh[//upload.-</p>

<p>wikimediaddorg/wikipedia/commons/thumb-</p>

<p>/d/dblef/Lock-gray-alt-24svg/9p-</p>

<p>it-Lock-gray-alt-24svgddpng["arbbback-</p>

<p>ground-image;conlinear-gradient"ghtransparent-</p>

<p>1transparent"areaurl"gh[//uploadddwikim-</p>

<p>ediaddorg/wikipedia/commons/d/dblef/-</p>

<p>lock-gray-alt-24svg["arbbbackground --</p>

<p>repeat;conno-repeat;bebackground-sizeccbleipxbbb-</p>

<p>ackground-position;conright dis1em</p>

<p>center_arddmw-parser-output</p>

<p>disid-lock-subscription aeaddmw-parser-output</p>

<p>discitation discsblea-lock-subscription</p>

<p>a^background-image;conurl"gh[//upload.-</p>

<p>wikimediaddorg/wikipedia/commons/thumb-</p>

<p>/a/aa/Lock-red-alt-24svg/9px--</p>

<p>lock-red-alt-24svgddpng["arbbbackgroun-</p>

<p>do-image;conlinear-gradient"ghtransparent;1tra-</p>

<p>nsparent"areaurl"gh[//uploadddwikimediaddo-</p>

<p>rg/wikipedia/commons/a/aa/Lock-rather-</p>

<p>ed-alt-24svg["arbbbackground-repeat;conn-</p>

<p>O-repeat;bebackground-sizeccbleipxbbbackgroun-</p>

<p>do-position;conright dis1em</p>

<p>center_arddmw-parser-output</p>

<p>discsblea-subscriptioneaddmw-parser-output</p>

<p>discsblea-registration^colorcc_thbleeee_arddmw-par-</p>

<p>ser-output discsblea-subscription</p>

<p>spaneaddmw-parser-output discsblea-registration</p>

<p>span^border-bottomccbleapx</p>

<p>dotted;becursor;conhelp_arddmw-parser-output</p>

<p>discsblea-ws-icon</p>

<p>a^background-image;conurl"gh[//upload.-</p>

<p>wikimediaddorg/wikipedia/commons/thumb-</p>

<p>/4/4c/Wikisource-logoddsvg/12-</p>

<p>px-Wikisource-logoddsvgddpng["arbbbackgr-</p>

<p>.do-image;conlinear-gradient"ghtransparent;1tr-</p>

<p>ansparent"areaurl"gh[//uploadddwikimedia.-</p>

<p>org/wikipedia/commons/4/4c/Wi-</p>

<p>kisource-logoddsvg["arbbbackground-repeat;-</p>

<p>conno-repeat;bebackground-sizeccbleabpxbbbackgr-</p>

<p>ound-position;conright dis1em</p>

<p>center_arddmw-parser-output</p>

<p>codeddcsblea-code^color;coninherit;bebackgr.-</p>

<p>d;coninherit;beborder;coninherit;bepadding;coninherit_-</p>

<p>arddmw-parser-output</p>

<p>discsblea-hidden-error^display;connone;beforeont --</p>

<p>sizeccbleajj.by _arddmw-parser-output</p>

<p>discsblea-visible-error^font-sizeccblea-  37</p>

<p>jj.by _arddmw-parser-output</p>

<p>discsblea-maint^display;connone;becolorcc_thblecc-</p>

<p>aableccbbmargin-leftccblejddcencem_arddmw-parser --</p>

<p>output discsblea-subscriptioneaddmw-parser-output</p>

<p>discsblea-registrationeaddmw-parser-output</p>

<p>discsblea-format^font-sizeccbleie.by _arddmw-parser-</p>

<p>comoutput discsblea-kern-lefteaddmw-parser-output</p>

<p>discsblea-kern-wl-left^padd±leftccblejddbencem-</p>

<p>_arddmw-parser-output</p>

<p>discsblea-kern-righteaddmw-parser-output</p>

<p>discsblea-kern-wl-right^padd±rightccblejddbencem_arddm-</p>

<p>will-parser-output discitation</p>

<p>dismw-selflink^font-weight;coninherit_ar. Lerner, R. G.; Trigg, G. L.</p>

<p>"ghbleaiia"ar. Encyclopaedia of Physics</p>

<p>"ghblebnd eddd"ar. VHC publishers.</p>

<p>ISBN 978-3-527-26954-9. Parker, C. B. "ghbleaiid"ar.   McGraw</p>

<p>Hill Encyclopaedia of Physics   "ghblebnd</p>

<p>eddd"ar. ISBN 978-0-07-051400-3. Lipschutz, S.; Lipson, M.</p>

<p>"ghblebjji"ar. Linear Algebra. Schaum's</p>

<p>Outlines "ghbledth eddd"ar. McGraw Hill</p>

<p>"ghUSA"ar. pp. 30-31.</p>

<p>ISBN 978-0-07-154352-1. Riley, K. F.; Hobson, M. P.;</p>

<p>Bence, S. J. "ghblebjaj"ar.</p>

<p>Mathematical methods for physics and engineering</p>

<p>. Cambridge University Press.</p>

<p>ISBN 978-0-521-86153-3. Adams, R. A. "ghbleaiie"ar. Calculus,</p>

<p>A Complete Course "ghblecrd eddd"ar.</p>

<p>Addison Wesley. p. 627.</p>

<p>ISBN 0"bjaherebhbcever. Horn, Johnson "ghblebjac"ar. Matrix</p>

<p>Analysis "ghblebnd eddd"ar. Cambridge</p>

<p>University Press. p. 6.</p>

<p>ISBN 978"j"521"edhbc"6. Lipcshutz, S.; Lipson, M.</p>

<p>"ghblebjji"ar. "2[. Linear Algebra.</p>

<p>Schaum's Outlines "ghbledth eddd"ar. McGraw</p>

<p>Hill "ghUSA"ar.</p>

<p>ISBN 978-0-07-154352-1. Horn, Johnson "ghblebjac"ar. "0[.</p>

<p>Matrix Analysis "ghblebnd eddd"ar.</p>

<p>Cambridge University Press.</p>

<p>ISBN 978"j"521"edhbc"6. Motwani, Rajeev; Raghavan, Prabhakar</p>

<p>"ghbleaiie"ar.  Randomized Algorithms .</p>

<p>Cambridge University Press.</p>

<p>p. 280. ISBN 9780521474658. Williams, Virginia Vassilevska.</p>

<p>"Multiplying matrices faster than</p>

<p>Coppersmith-Winograd" "ghPDF"ar. Le Gall, Fran~andcois "ghblebjad"ar,
"Powers</p>

<p>of tensors and fast matrix multiplication",</p>

<p>Proceedings of the 39th International</p>

<p>Symposium on Symbolic and Algebraic</p>

<p>Computation "ghISSAC 2014"ar, arXiv:</p>

<p>140147714 ,</p>

<p>Bibcodeccblebjad;arXivbleadjaddggadL Raz, Ran "ghJanuary 2003"ar. "On
the</p>

<p>Complexity of Matrix Product". SIAM</p>

<p>Journal on Computing. ~132 "ghblee"ar:</p>

<p>1356-1369.</p>

<p>doiccbleajddaacg/sblejjigecigjbdjbadg.</p>

<p>ISSN 0097-5397.</p>

<p>40</p>

<p>References</p>

<p> be  Wikimedia Commons has media related to</p>

<p>matrix multiplication   .;  . be  The Wikibook  Linear Algebra  has a</p>

<p>page on the topic of:   Matrix</p>

<p>multiplication   be  . be  The Wikibook  Applicable Mathematics</p>

<p>has a page on the topic of:   Multiplying</p>

<p>Matrices   be  . dismw-parser-output disrefbegin^font-sizeccbleij.by
bemargin-bottomccblejddeencem_arddmw-parser-output
disrefbegin-hang±indents`arul^list-style-type;connone;bemargin-leftccblej_arddmw-parser-output
disrefbegin-hang±indents`arul`arlieaddmw-parser-output
disrefbegin-hang±indents`ardl`ardd^margin-leftccblejbbpadding-leftccblecddbencembbtext-indent:comblecddbencembblist-style;connone_arddmw-parser-output
disrefbegin-100^font-sizeccbleajj.by _ar Henry Cohn, Robert Kleinberg,
Bal~stazs</p>

<p>Szegedy, and Chris Umans. Group-theoretic</p>

<p>Algorithms for Matrix Multiplication.</p>

<p>arXiv;conmathddGR/0511460.</p>

<p>Proceedings of the 46th Annual Symposium</p>

<p>on Foundations of Computer Science,</p>

<p>23-25 October 2005, Pittsburgh,</p>

<p>PA, IEEE Computer Society,</p>

<p>pp. 379-388. Henry Cohn, Chris Umans. A</p>

<p>Group-theoretic Approach to Fast Matrix</p>

<p>Multiplication.</p>

<p>arXiv;conmathddGR/0307321. Proceedings of</p>

<p>the 44th Annual IEEE Symposium on</p>

<p>Foundations of Computer Science, 11-14</p>

<p>October 2003, Cambridge, MA,</p>

<p>IEEE Computer Society,</p>

<p>pp. 438-449. Coppersmith, D.; Winograd, S.</p>

<p>"ghbleaiij"ar. "Matrix multiplication via</p>

<p>arithmetic progressions". J. Symbolic</p>

<p>Comput. ~29 "ghblec"ar: 251-280.</p>

<p>doiccbleajddajaf/sblejgdg-7171"ghblejh"arblehjj-</p>

<p>according-2. Horn, Roger A.; Johnson, Charles R.</p>

<p>"ghbleaiia"ar, Topics in Matrix</p>

<p>Analysis, Cambridge University</p>

<p>Press, ISBN 978-0-521-46713-1 Knuth, DddE.,  The Art of Computer</p>

<p>Programming Volume 2: Seminumerical</p>

<p>Algorithms. Addison-Wesley</p>

<p>Professional; 3 edition "ghationovember 14,</p>

<p>1997"ar.</p>

<p>ISBN 978-0-201-89684-8.</p>

<p>pp. 501. Press, William H.; Flannery, Brian</p>

<p>P.; Teukolsky, Saul A.; Vetterling,</p>

<p>William T. "ghblebjjg"ar,  Numerical</p>

<p>Recipes: The Art of Scientific Computing</p>

<p>"ghblecrd eddd"ar, Cambridge University</p>

<p>Press, ISBN 978-0-521-88068-8</p>

<p>. Ran Raz. On the complexity of matrix</p>

<p>product. In Proceedings of the thirty-fourth</p>

<p>annual ACM symposium on Theory of</p>

<p>computing. ACM Press, 2002.</p>

<p>doiccbleajddaade/5099074509932. Robinson, Sara, Toward an Optimal</p>

<p>Algorithm for Matrix Multiplication,</p>

<p>SIAM News 38"ghblei"ar, November</p>

<p>2005. PDF Strassen, Volker, Gaussian Elimination is</p>

<p>not Optimal, Numer. Math. 13,</p>

<p>p. 354-356, 1969. Styan, George P. H. "ghbleaigc"ar,</p>

<p>"Hadamard Products and Multivariate</p>

<p>Statistical Analysis" "ghPDF"ar, Linear</p>

<p>Algebra and its Applications, ~26:</p>

<p>217-240,</p>

<p>doiccbleajddajaf/0024-3795"ghblegc"arbleijjb-</p>

<p>can-2 Williams, Virginia Vassilevska</p>

<p>"ghblebjab-05-19"ar. "Multiplying</p>

<p>matrices faster than coppersmith-winograd".</p>

<p>Proceedings of the 44th symposium on Theory</p>

<p>of Computing - Stoc '12. acm.</p>

<p>pp. 887-898. CiteSeerX</p>

<p>104141429742680 .</p>

<p>doiccbleajddaade/221397742214056.</p>

<p>ISBN 9781450312455. v t e Algebra   . Areas Abstract algebra Category
theory Elementary algebra K-theory Commutative algebra Noncommutative
algebra Order theory Universal algebra</p>

<p>be  . Algebraic structures Group "ghtheory"ar Ring "ghtheory"ar Module
"ghtheory"ar Field Polynomial ring "ghPolynomial"ar Composition
algebra</p>

<p>be  . Linear algebra Matrix "ghtheory"ar Vector space "ghVector"ar
Module Inner product space "ghdot product"ar Hilbert space</p>

<p>be  . Multilinear algebra Tensor algebra Exterior algebra Symmetric
algebra Geometric algebra "ghMultivector"ar  45</p>

<p>be  . Topic lists Abstract algebra Algebraic structures Group theory
Linear algebra</p>

<p>be  . Glossaries Linear algebra Field theory Ring theory Order theory</p>

<p>be  . Related Mathematics History of algebra</p>

<p>be  . Category Mathematics portal Wikibooks Elementary Linear Abstract
Wikiversity Linear Abstract be  .</p>

<p>Retrieved from
"httpscc//enddwikipediaddorg/w/indexddphp;8title"7Matrix.commultiplication&amp;oldid"7969667138[</p>

<p>47</p>

</body></html>
